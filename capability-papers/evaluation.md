## Agent Evaluation

<figure style="text-align: center;">
    <img alt="" src="../assets/evaluation.png" width="500" />
    <figcaption style="text-align: center;">ÂºïÁî®Ôºöhttps://arxiv.org/abs/2407.18961</figcaption>
</figure>

#### Papers
* ‚öñÔ∏è [Aug 2023] **"AgentBench: Evaluating LLMs as Agents"** [[paper](https://arxiv.org/abs/2308.03688)]
* ‚öñÔ∏è [Oct 2023] **"SmartPlay: A Benchmark for LLMs as Intelligent Agents"** [[paper](https://arxiv.org/abs/2310.01557)]
* ‚öñÔ∏è [Nov 2023] **"MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration"** [[paper](https://arxiv.org/abs/2311.08562)]
* ‚öñÔ∏è [Nov 2023] **"GAIA: A Benchmark for General AI Assistants"** [[paper](https://arxiv.org/abs/2311.12983)]
* ‚öñÔ∏è [Dec 2023] **"Evaluating Language-Model Agents on Realistic Autonomous Tasks"** [[paper](https://arxiv.org/abs/2312.11671)]
* ‚öñÔ∏è [Jan 2024] **"AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents"** [[paper](https://arxiv.org/abs/2401.13178)]
* ‚öñÔ∏è [Feb 2024] **"AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"** [[paper](https://arxiv.org/abs/2402.08995)]
* üìñ [Apr 2024] **"Hallucination of Multimodal Large Language Models: A Survey"** [[paper](https://arxiv.org/abs/2404.18930)]
* ‚öñÔ∏è [May 2024] **"Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View"** [[paper](https://arxiv.org/abs/2405.14744)]
* ‚öñÔ∏è [Jun 2024] **"The BiGGen Bench: A Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models"** [[paper](https://arxiv.org/abs/2406.05761)]
* üìñ [Jun 2024] **"A Survey of Useful LLM Evaluation"** [[paper](https://arxiv.org/abs/2406.00936)]
* [Jul 2024] **"AI Agents That Matter"** [[paper](https://arxiv.org/abs/2407.01502)]
* üî• ‚öñÔ∏è [Jul 2024] **"MMAU: A Holistic Benchmark of Agent Capabilities Across Diverse Domains"** [[paper](https://arxiv.org/abs/2407.18961)]
* ‚öñÔ∏è [Aug 2024] **"VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents"** [[paper](https://arxiv.org/abs/2408.06327)]
* üìñ [Aug 2024] **"A Survey on Evaluation of Multimodal Large Language Models"** [[paper](https://arxiv.org/abs/2408.15769)]
* ‚öñÔ∏è [Sep 2024] **"Evaluation of OpenAI o1: Opportunities and Challenges of AGI"** [[paper](https://arxiv.org/abs/2409.18486)]
* üìñ [Sep 2024] **"The Imperative of Conversation Analysis in the Era of LLMs: A Survey of Tasks, Techniques, and Trends"** [[paper](https://arxiv.org/abs/2409.14195)]
* ‚öñÔ∏è [Oct 2024] **"BENCHAGENTS: Automated Benchmark Creation with Agent Interaction"** [[paper](https://arxiv.org/abs/2410.22584)]
* [Nov 2024] **"Evaluating World Models with LLM for Decision Making"** [[paper](https://arxiv.org/abs/2411.08794)]
* [Dec 2024] **"MISR: Measuring Instrumental Self-Reasoning in Frontier Models"** [[paper](https://arxiv.org/abs/2412.03904)]
* ‚öñÔ∏è [Dec 2024] **"RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios"** [[paper](https://arxiv.org/abs/2412.08972)]
* ‚öñÔ∏è [Dec 2024] **"EscapeBench: Pushing Language Models to Think Outside the Box"** [[paper](https://arxiv.org/abs/2412.13549)]
* ‚öñÔ∏è [Dec 2024] **"TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks"** [[paper](https://arxiv.org/abs/2412.14161)]
* ‚öñÔ∏è [Mar 2025] **"WritingBench: A Comprehensive Benchmark for Generative Writing"** [[paper](https://arxiv.org/abs/2503.05244)]
* ‚öñÔ∏è [Mar 2025] **"Writing as a testbed for open ended agents"** [[paper](https://arxiv.org/abs/2503.19711)]
* ‚öñÔ∏è [Mar 2025] **"Measuring AI Ability to Complete Long Tasks"** [[paper](https://arxiv.org/abs/2503.14499)]
* üìñ [Mar 2025] **"Survey on Evaluation of LLM-based Agents"** [[paper](https://arxiv.org/abs/2503.16416)]
* üìñ [Mar 2025] **"Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey"** [[paper](https://arxiv.org/abs/2503.22458)]
* ‚öñÔ∏è [Apr 2025] **"AGENTREWARDBENCH: Evaluating Automatic Evaluations of Web Agent Trajectories"** [[paper](https://arxiv.org/abs/2504.08942)]
* ‚öñÔ∏è [May 2025] **"Benchmarking LLMs‚Äô Swarm intelligence"** [[paper](https://www.arxiv.org/abs/2505.04364)]
* ‚öñÔ∏è [May 2025] **"On Path to Multimodal Generalist: General-Level and General-Bench"** [[apper](https://arxiv.org/abs/2505.04620)]
* ‚öñÔ∏è [May 2025] **"AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios"** [[paper](https://arxiv.org/abs/2505.16944)]
* ‚öñÔ∏è [May 2025] **"Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models"** [[paper](https://arxiv.org/abs/2505.02847)]
* ‚öñÔ∏è [Jun 2025] **"AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models"** [[paper](https://arxiv.org/abs/2506.11110)]
* ‚öñÔ∏è [Jun 2025] **"Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents"** [[paper](https://arxiv.org/abs/2506.21252)]
* ‚öñÔ∏è [Jul 2025] **"MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models"** [[paper](https://arxiv.org/abs/2507.12806)]
* ‚öñÔ∏è [Jul 2025] **"Evaluation and Benchmarking of LLM Agents: A Survey"** [[paper](https://arxiv.org/abs/2507.21504)]
* ‚öñÔ∏è [Jul 2025] **"UserBench: An Interactive Gym Environment for User-Centric Agents"** [[paper](https://arxiv.org/abs/2507.22034)]
* ‚öñÔ∏è [Jul 2025] **"CLEAR: Error Analysis via LLM-as-a-Judge Made Easy"** [[paper](https://arxiv.org/abs/2507.18392)]
* [Aug 2025] **"Efficient Agents: Building Effective Agents While Reducing Cost"** [[paper](https://www.arxiv.org/abs/2508.02694)]
* [Aug 2025] **"Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks"** [[paper](https://arxiv.org/abs/2508.13143)]
* ‚öñÔ∏è [Aug 2025] **"FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction"** [[paper](https://arxiv.org/abs/2508.11987)]
* ‚öñÔ∏è [Aug 2025] **"UQ: Assessing Language Models on Unsolved Questions"** [[paper](https://arxiv.org/abs/2508.17580)]
* [Aug 2025] **"How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on œÑ-bench"** [[paper](https://arxiv.org/abs/2508.20931)]
* [Sep 2025] **"JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer"** [[paper](https://arxiv.org/abs/2509.02097)]
* ‚öñÔ∏è [Sep 2025] **"HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants"** [[paper](https://arxiv.org/abs/2509.08494)]
* ‚öñÔ∏è [Sep 2025] **"Rationality Check! Benchmarking the Rationality of Large Language Models"** [[paper](https://arxiv.org/abs/2509.14546)]
* ‚öñÔ∏è [Sep 2025] **"GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks"** [[paper](https://openai.com/index/gdpval/)]
* ‚öñÔ∏è [Sep 2025] **"ARE: Scaling Up Agent Environments and Evaluations"** [[paper](http://arxiv.org/abs/2509.17158)]
* ‚öñÔ∏è [Sep 2025] **"AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise"** [[paper](https://www.arxiv.org/abs/2509.10769)]
* ‚öñÔ∏è [Sep 2025] **"ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs"** [[paper](https://arxiv.org/abs/2510.00857)]