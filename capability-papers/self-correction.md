## Self-Correction
These papers discuss whether agents can autonomously recover from errors.

#### Papers
* [Jun 2022] **"Self-critiquing models for assisting human evaluators"** [[paper](https://arxiv.org/abs/2206.05802)]
* [Mar 2023] **"Reflexion: Language Agents with Verbal Reinforcement Learning"** [[paper](https://arxiv.org/abs/2303.11366)]
* [Mar 2023] **"Self-Refine: Iterative Refinement with Self-Feedback"** [[paper](https://arxiv.org/abs/2303.17651)]
* [Apr 2023] **"Teaching Large Language Models to Self-Debug"** [[paper](https://arxiv.org/abs/2304.05128)]
* [May 2023] **"CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing"** [[paper](https://arxiv.org/abs/2305.11738)]
* üìñ [Aug 2023] **"Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies"** [[paper](https://arxiv.org/abs/2308.03188)]
* [Nov 2023] **"The ART of LLM Refinement: Ask, Refine, and Trust"** [[paper](https://arxiv.org/abs/2311.07961)]
* ‚öñÔ∏è [Dec 2023] **"LLF-Bench: Benchmark for Interactive Learning from Language Feedback"** [[paper](https://arxiv.org/abs/2312.06853)]
* [Jan 2024] **"Towards Uncertainty-Aware Language Agent"** [[paper](https://arxiv.org/abs/2401.14016)]
* [Jan 2024] **"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"** [[paper](https://arxiv.org/abs/2401.00812)]
* [Feb 2024] **"Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models"** [[paper](https://arxiv.org/abs/2402.12563)]
* [Feb 2024] **"Large Language Models Cannot Self-Correct Reasoning Yet"** [[paper](https://arxiv.org/abs/2310.01798)]
* [Feb 2024] **"On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks"** [[paper](https://arxiv.org/abs/2402.08115)]
* [May 2024] **"Devil's Advocate: Anticipatory Reflection for LLM Agents"** [[paper](https://arxiv.org/abs/2405.16334)]
* [May 2024] **"Self-Reflection in LLM Agents: Effects on Problem-Solving Performance"** [[paper](https://arxiv.org/abs/2405.06682)]
* [Jun 2024] **"Devil‚Äôs Advocate: Anticipatory Reflection for LLM Agents"** [[paper](https://arxiv.org/abs/2405.16334)]
* [Jun 2024] **"Luban: Building Open-Ended Creative Agents via Autonomous Embodied Verification"** [[paper](https://arxiv.org/abs/2405.15414)]
* üìñ [Jun 2024] **"When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs"** [[paper](https://arxiv.org/abs/2406.01297)]
* [Jul 2024] **"Direct-Inverse Prompting: Analyzing LLMs‚Äô Discriminative Capacity in Self-Improving Generation"** [[paper](https://arxiv.org/abs/2407.11017)]
* üìñ [Aug 2024] **"Internal Consistency and Self-Feedback in Large Language Models: A Survey"** [[paper](https://arxiv.org/abs/2407.14507)]
* üî• ‚öñÔ∏è [Oct 2024] **"Reflection-Bench: Probing AI Intelligence with Reflection"** [[paper](https://arxiv.org/abs/2410.16270)]
* [Sep 2024] **"CoT Rerailer: Enhancing the Reliability of Large Language Models in Complex Reasoning Tasks through Error Detection and Correction"** [[paper](https://arxiv.org/abs/2408.13940)]
* [Sep 2024] **"An Empirical Study on Self-correcting Large Language Models for Data Science Code Generation"** [[paper](https://arxiv.org/abs/2408.15658)]
* [Oct 2024] **"Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation"** [[paper](https://arxiv.org/abs/2410.05801)]
* [Oct 2024] **"Agent-as-a-Judge: Evaluate Agents with Agents"** [[paper](https://arxiv.org/abs/2410.10934)]
* [Oct 2024] **"LLM Self-Correction with DeCRIM: Decompose, Critique, and Refine for Enhanced Following of Instructions with Multiple Constraints"** [[paper](https://arxiv.org/abs/2410.06458)]
* [Nov 2024] **"From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge"** [[paper](https://arxiv.org/abs/2411.16594)]
* [Dec 2024] **"Meta-Reflection: A Feedback-Free Reflection Learning Framework"** [[paper](https://arxiv.org/abs/2412.13781)]
* [Dec 2024] **"Understanding the Dark Side of LLMs‚Äô Intrinsic Self-Correction"** [[paper](https://arxiv.org/abs/2412.14959)]
* ‚öñÔ∏è [Jan 2025] **"RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques"** [[paper](https://arxiv.org/abs/2501.14492)]
* [Jan 2025] **"Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge"** [[paper](https://arxiv.org/abs/2501.18099)]
* [Feb 2025] **"Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers"** [[paper](https://arxiv.org/abs/2502.20379)]
* [Apr 2025] **"Review, Refine, Repeat: Understanding Iterative Decoding of AI Agents with Dynamic Evaluation and Selection"** [[paper](https://arxiv.org/abs/2504.01931)]
* [Apr 2025] **"T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models"** [[paper](https://arxiv.org/abs/2504.04718)]
* [Apr 2025] **"Agentic Knowledgeable Self-awareness"** [[paper](https://arxiv.org/abs/2504.03553)]
* [Apr 2025] **"Adaptive Rectification Sampling for Test-Time Compute Scaling"** [[paper](https://arxiv.org/abs/2504.01317)]
* üìñ [Apr 2025] **"Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey"** [[paper](https://arxiv.org/abs/2504.14520)]
* [Apr 2025] **"Leveraging LLMs as Meta-Judges: A Multi-Agent Framework for Evaluating LLM Judgments"** [[paper](https://www.arxiv.org/abs/2504.17087)]
* [May 2025] **"AutoLibra: Agent Metric Induction from Open-Ended Feedback"** [[paper](https://arxiv.org/abs/2505.02820)]
* [May 2025] **"ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection"** [[paper](https://arxiv.org/abs/2505.15182)]
* [May 2025] **"Reflect, Retry, Reward:Self-Improving LLMs via Reinforcement Learning"** [[paper](https://arxiv.org/abs/2505.24726)]
* [Jun 2025] **"Incentivizing LLMs to Self-Verify Their Answers"** [[paper](https://arxiv.org/abs/2506.01369)]
* [Jul 2025] **"Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains"** [[paper](https://arxiv.org/abs/2507.17746)]
* [Jul 2025] **"Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation"** [[paper](https://arxiv.org/abs/2507.21028)]
* ‚öñÔ∏è [Jul 2025] **"CLEAR: Error Analysis via LLM-as-a-Judge Made Easy"** [[paper](https://arxiv.org/abs/2507.18392)]
* ‚öñÔ∏è [Aug 2025] **"Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation"** [[paper](https://www.arxiv.org/abs/2508.05508)]
* ‚öñÔ∏è [Aug 2025] **"CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward"** [[paper](https://arxiv.org/abs/2508.03686)]
* üìñ [Aug 2025] **"When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs"** [[paper](https://arxiv.org/abs/2508.02994)]
* [Aug 2025] **"Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning"** [[paper](https://www.arxiv.org/abs/2508.01543)]
* [Sep 2025] **"Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers"** [[paper](https://www.arxiv.org/abs/2509.03059)]
* [Sep 2025] **"SAMULE: Self-Learning Agents Enhanced by Multi-level Reflection"** [[paper](https://arxiv.org/abs/2509.20562)]
* [Sep 2025] **"Who‚Äôs Your Judge? On the Detectability of LLM-Generated Judgments"** [[paper](https://arxiv.org/abs/2509.25154)]
* üìñ [Oct 2025] **"A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models"** [[paper](https://arxiv.org/abs/2510.08049)]