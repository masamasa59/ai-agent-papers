## 知覚
### Text
* [Jun 2024] **"Ask-before-Plan: Proactive Language Agents for Real-World Planning"** [[paper](https://arxiv.org/abs/2406.12639)]
* [May 2024] **"In-Context Learning with Long-Context Models: An In-Depth Exploration"** [[paper](https://arxiv.org/abs/2405.00200)]
* [May 2024] **"Many-Shot In-Context Learning"** [[paper](https://arxiv.org/abs/2404.11018)]
* ⚖️ [May 2024] **"Elements of World Knowledge (EWOK): A cognition-inspired framework for evaluating basic world knowledge in language models"** [[paper](https://arxiv.org/abs/2405.09605)]
* [Jun 2024] **Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA"** [[paper](https://arxiv.org/abs/2406.17419)]
* [Jun 2024] **LLM In-Context Recall is Prompt Dependent"** [[paper](https://arxiv.org/abs/2404.08865)]
* [Jun 2024] **BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack"** [[paper](https://arxiv.org/abs/2406.10149)]
* [Jun 2024] **Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?"** [[paper](https://arxiv.org/abs/2406.13121)]
* [Jun 2024] **Are Long-LLMs A Necessity For Long-Context Tasks?"** [[paper](https://arxiv.org/abs/2405.15318)]
* [Jun 2024] **Chain of Agents: Large Language Models Collaborating on Long-Context Tasks"** [[paper](https://arxiv.org/abs/2406.02818)]
* [Jun 2024] **Hello Again! LLM-powered Personalized Agent for Long-term Dialogue"** [[paper](https://arxiv.org/abs/2406.05925)]
* [Jul 2024] **Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems"** [[paper](https://arxiv.org/abs/2407.01370)]
* [Nov 2024] **"Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?"** [[paper](https://arxiv.org/abs/2411.05000)]
### Image
* ⚖️ [Feb 2024] **"PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain"** [[paper](https://arxiv.org/abs/2402.15527)]
* [Feb 2024] **"L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects"** [[paper](https://arxiv.org/abs/2402.09052)]
* [May 2024] **"Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"** [[paper](https://arxiv.org/abs/2403.05530)]
* [May 2024] **"Many-Shot In-Context Learning in Multimodal Foundation Models"** [[paper](https://arxiv.org/abs/2405.09798)]
* [Jun 2024] **Needle In A Multimodal Haystack"** [[paper](https://arxiv.org/abs/2406.07230)]
* [Jun 2024] **Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models"** [[paper](https://arxiv.org/abs/2406.11230)]
* [Jun 2024] **CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs"** [[paper](https://arxiv.org/abs/2406.18521)]
* ⚖️ [Nov 2024] **"M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework"** [[paper](https://arxiv.org/abs/2411.06176)]
### Movie
* [May 2024] **"CinePile: A Long Video Question Answering Dataset and Benchmark"** [[paper](https://arxiv.org/abs/2405.08813)]
* [May 2024] **Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis"** [[paper](https://arxiv.org/abs/2405.21075)]
* [Jun 2024] **DrVideo: Document Retrieval Based Long Video Understanding"** [[paper](https://arxiv.org/abs/2406.12846)]
* [Oct 2024] **"Adaptive Video Understanding Agent: Enhancing efficiency with dynamic frame sampling and feedback-driven reasoning"** [[paper](https://arxiv.org/abs/2410.20252)]
